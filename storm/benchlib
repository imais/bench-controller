#!/bin/bash

source ./bench.conf

STORM_BENCH_JAR="${STORM_BENCH_HOME}/target/storm-bench-0.1-jar-with-dependencies.jar"
CONF="${STORM_BENCH_HOME}/conf/conf.yaml"
PACKAGE="rpi.storm.benchmark"
CORES_PER_VM=2

SAMOA_JAR="${SAMOA_HOME}/target/SAMOA-Storm-0.5.0-incubating-SNAPSHOT.jar"

NUM_TESTS=3     # test per app
NUM_SAMPLING=3  # sampling per test
SAMPLING_PERIOD_SEC=5
SAMPLING_INTERVAL_SEC=5
MAX_THROUGHPUT_MONITORING_SEC=120

HOSTS_FILE=hosts
TCPDUMP_FILTER=tcpdump_filter
TCPDUMP="${REMOTE_DIR}/tcpdump"
SYSMON="${REMOTE_DIR}/sysmon"

POST_RUN_SUPERVISOR_WAIT_SEC=20
POST_KILL_SUPERVISOR_WAIT_SEC=10
POST_RUN_TOPO_WAIT_SEC=20
POST_KILL_TOPO_WAIT_SEC=15

# Sampling convergence condition:
#   If the latest monitored value is within WITHIN_X_PERCENT[%] 
#   compared to the previous monitored value for K times out of last N times,
#   we determine the value is converged
WITHIN_X_PERCENT=5
N=5
K=4


###### misc functions

function join_by { local IFS="$1"; shift; echo "$*"; }
function get_date { date "+%Y-%m-%d %H:%M:%S"; }
function get_date_sec { date "+%s"; }
function dbg_print { echo "`get_date`" $1; }
function run_cmd { dbg_print "$1"; eval $1; }
function run_pssh {
    local remote_cmd=$1
    local cmd="pssh -l ${REMOTE_USER} -h ${HOSTS_FILE} \"${remote_cmd}\""
    run_cmd "$cmd"
}
function get_rand { 
    awk -v seed=$RANDOM 'BEGIN { srand(seed); printf("%.5f\n", rand() * 100.0) }';
}

# queue using an array
# http://www.tech-recipes.com/rx/911/queue-and-stack-using-array/
queue=()
function queue_add {
    # add an element to the end of the array
    queue=(${queue[@]} $1)
}
function queue_remove {
    # remove the first element
    queue=(${queue[@]:1:$((${#queue[@]}))})
}
function queue_remove_all {
    queue=()
}
function queue_show_all {
    echo ${queue[@]}
}
function queue_len {
    echo ${#queue[@]}
}


###### run_supervisors & kill_supervisors

function run_supervisors {
    local m
    if [ $# -eq 0 ] || [ ${#1} -gt 3 ] ; then
        dbg_print "Usage: ./run_supervisor [1-128]"
        exit 1
    elif [ ${#1} -eq 1 ] ; then
        m=00$1
    elif [ ${#1} -eq 2 ] ; then
        m=0$1
    elif [ ${#1} -eq 3 ] ; then
        m=$1
    fi
    dbg_print "Starting $m supervisors..."
    run_cmd "pdsh -R ssh -l ${REMOTE_USER} -w slave[001-$m] 'sudo rm -rf /app/storm; sudo mkdir -p /app/storm; sudo chown -R ${REMOTE_USER}:${REMOTE_USER} /app/storm; /usr/local/storm/bin/storm supervisor < /dev/null > /home/${REMOTE_USER}/supervisor.log 2>&1 &'"
    echo $m > ./supervisors
}


function kill_supervisors {
    local m
    m=`cat ./supervisors`
    dbg_print "Killing ${m} supervisors..."
    run_cmd "pdsh -R ssh -l ${REMOTE_USER} -w slave[001-$m] 'pkill java'"
}


###### run_topo & kill_topo

function run_topo {
    if [ $# -ne 2 ] ; then
        dbg_print "Usage: run_topo {rsort|sol|wc|clean|grep|pv|visitor|rcount|trident|vhtree} [#VMs(m)]"
        exit 1    
    fi

    local class
    local topo
    local topic
    local num_comps

    local is_samoa=false
    local num_classes

    local app=$1
    local m=$2

    case "$app" in
        rsort)
            class=RollingSort
            topo=rollingsort
            topic=rand
            num_comps=2
            ;;
        sol)
            class=SOL
            topo=sol
            topic=bytes
            num_comps=3
            ;;
        wc)
            class=WordCount
            topo=wordcount
            topic=book
            num_comps=3
            ;;
        clean)
            class=DataClean
            topo=dataclean
            topic=view
            num_comps=3
            ;;
        grep)
            class=Grep
            topo=grep
            topic=book
            num_comps=3
            ;;
        pv)
            class=PageViewCount
            topo=pageviewcount
            topic=view
            num_comps=3
            ;;
        visitor)
            class=UniqueVisitor
            topo=uniquevisitor
            topic=view
            num_comps=3
            ;;
        rcount)
            class=RollingCount
            topo=rollingcount
            topic=book
            num_comps=3
            ;;
        trident)
            class=TridentWordCount
            topo=tridentcount
            topic=book
            num_comps=3
            ;;
        vhtree)
            topo=vhtree
            # topic=covtype
            # num_classes=7
            topic=rand-tree
            num_classes=10
            is_samoa=true
    esac

    dbg_print "Submitting ${topo}..."
    
    if [ ${is_samoa} == "false" ] ; then
        # Intel storm bench
        local parallel=$(($m * ${CORES_PER_VM} / ${num_comps}))
        if [ $parallel -lt 1 ] ; then
            parallel=1
        fi;
        local workers=$m
        local ackers=$m

        run_cmd "storm jar ${STORM_BENCH_JAR} ${PACKAGE}.${class} ${topo} -conf ${CONF} -topic ${topic} -parallel ${parallel} -workers ${workers} -ackers ${ackers}"
    else
        # SAMOA - only works for PrequentialEvaluation for the moment
        local parallel=$(($m - 4))
        if [ $parallel -lt 1 ] ; then
            parallel=1
        fi;
        export SAMOA_STORM_NUMWORKER=$m

        run_cmd "${SAMOA_HOME}/bin/samoa storm ${SAMOA_JAR} \"PrequentialEvaluation -d /tmp/dump.csv -i -1 -f 100000 -n ${topo} -l (classifiers.trees.VerticalHoeffdingTree -p ${parallel}) -s (org.apache.samoa.streams.kafka.KafkaStream -r 20 -t ${topic} -k $((${num_classes}+1)) -s ${KAFKA_HOST})\""
        # run_cmd "${SAMOA_HOME}/bin/samoa storm ${SAMOA_JAR} \"PrequentialEvaluation -d /tmp/dump.csv -i -1 -f 100000 -n ${topo} -l (classifiers.trees.VerticalHoeffdingTree -p ${parallel}) -s (generators.RandomTreeGenerator -c 10 -o 100 -u 100)\""
    fi

    echo ${topo} > ./topo
}


function kill_topo {
    local topo
    case "$1" in
        rsort)
            topo=rollingsort
            ;;
        sol)
            topo=sol
            ;;
        wc)
            topo=wordcount
            ;;
        clean)
            topo=dataclean
            ;;
        grep)
            topo=grep
            ;;
        pv)
            topo=pageviewcount
            ;;
        visitor)
            topo=uniquevisitorp
            ;;
        trident)
            topo=tridentwordcount
            ;;
        rcount)
            topo=rollingcount
            ;;
        trident)
            topo=trident
            ;;
        vhtree)
            topo=vhtree
            ;;
        *)
            topo=`cat ./topo`
            ;;
    esac

    dbg_print "Killing ${topo}..."
    run_cmd "storm kill -w 0 ${topo} || true"
}


###### sample_metrics

function sample_metrics {
    local output_dir
    if [ $# -eq 0 ] ; then
        output_dir="output"
    else
        output_dir=$1
    fi
    
    dbg_print "Start sampling (output_dir: ${output_dir})"
    run_cmd "ssh ${REMOTE_USER}@${TCPDUMP_HOST} \"sudo ${TCPDUMP} -i eth0 -Q out -F ${TCPDUMP_FILTER} --throughput-tracking-mode >& tcpdump.log &\""
    run_pssh "sudo ${SYSMON} 0 ${REMOTE_DIR}/sysmon.log 0x48040020 10000 2 > /dev/null &"

    dbg_print 'Monitoring in progress...'
    sleep ${SAMPLING_PERIOD_SEC}

    run_cmd "ssh ${REMOTE_USER}@${TCPDUMP_HOST} \"sudo pkill --signal 2 tcpdump\""
    run_pssh "sudo pkill --signal 2 sysmon"
    dbg_print 'Collecting metrics...'

    if [ ! -d "${output_dir}" ]; then
        mkdir -p ${output_dir}
    else
        rm -rf ${output_dir}/*
    fi

    run_cmd "pslurp -l ${REMOTE_USER} -h ${HOSTS_FILE} -L ${output_dir} ${REMOTE_DIR}/sysmon.log ."
    run_cmd "scp ${REMOTE_USER}@${TCPDUMP_HOST}:${REMOTE_DIR}/tcpdump.log ${output_dir}/"
}


###### monitor_throughput

function get_throughput {
   if [ $# -ne 1 ] ; then
        dbg_print "Usage: get_throughput [output dir]"
        exit 1
   fi

   local output_dir=$1

   eval "ssh ${REMOTE_USER}@${TCPDUMP_HOST} \"sudo ${TCPDUMP} -i eth0 -Q out -F ${TCPDUMP_FILTER} --throughput-tracking-mode >& tcpdump.log &\""
   sleep ${SAMPLING_PERIOD_SEC}
   eval "ssh ${REMOTE_USER}@${TCPDUMP_HOST} \"sudo pkill --signal 2 tcpdump\""
   eval "scp ${REMOTE_USER}@${TCPDUMP_HOST}:${REMOTE_DIR}/tcpdump.log ${output_dir}/"
   echo `tail -n1 ${output_dir}/tcpdump.log | cut -d' ' -f9`
}

function is_within_x_percent {
    local prev=$1
    local curr=$2

    # https://stackoverflow.com/questions/806906/how-do-i-test-if-a-variable-is-a-number-in-bash
    local re='^[0-9]+([.][0-9]+)?$'

    # if [ $prev -eq 0 ] || [ $curr -eq 0 ] ; then
    if (( $(bc <<< "$prev == 0.0") )) || (( $(bc <<< "$curr == 0.0") )) ; then
        echo 0
    elif ! [[ $prev =~ $re ]] || ! [[ $curr =~ $re ]] ; then
        echo 0
    else
        local delta=$(eval "awk 'function abs(v) {return v < 0 ? -v : v}; BEGIN {printf \"%.5f\", 100*abs(($prev)-($curr))/abs($prev)}'")
        if (( $(bc <<< "$delta < ${WITHIN_X_PERCENT}") )) ; then
            echo 1
        else
            echo 0
        fi
    fi
}

function monitor_throughput {
   if [ $# -ne 1 ] ; then
        dbg_print "Usage: monitor_throughput [output dir]"
        exit 1
   fi
   
   local output_dir=$1
   local prev=0.0
   local curr=0.0
   local t=0
   local converged=0
   local log=${output_dir}/convergence.log

   if [ -f $log ] ; then
       rm $log
   fi

   queue_remove_all
   while [ $converged -eq 0 ] && [ $t -lt ${MAX_THROUGHPUT_MONITORING_SEC} ] ; do
       curr=`get_throughput ${output_dir}`
       t=$(($t + ${SAMPLING_INTERVAL_SEC}))

       within_x_percent=`is_within_x_percent $prev $curr`
       queue_add ${within_x_percent}

       if [ `queue_len` -gt $N ] ; then
           queue_remove
       fi

       # K out N check
       sum=0
       for val in "${queue[@]}" ; do
           sum=$(($sum + $val))
       done

       if [ $sum -ge $K ] ; then
           # let's get out of the loop
           converged=1
       fi

       msg="$t, $curr, $sum, $converged"
       dbg_print "$msg"
       echo $msg >> $log

       prev=$curr
   done
}


###### run_test

function create_hosts_file {
    if [ $# -ne 1 ] ; then
        dbg_print "Usage: create_hosts_file [#VMs(m)]"
        exit 1
    fi

    local m=$1

    dbg_print "Creating ${HOSTS_FILE} file for ${m} VMs"

    if [ -f ${HOSTS_FILE} ] ; then 
        rm ${HOSTS_FILE}
    fi
    touch ${HOSTS_FILE}

    local host i
    for host in ${HOSTS[@]}; do
        echo $host >> ${HOSTS_FILE}
    done
    for ((i=1; i <= $m; i++)) ; do
        printf "slave%03d\n" $i >> ${HOSTS_FILE}
    done
}


function create_tcpdump_filter {
    if [ $# -ne 1 ] ; then
        dbg_print "Usage: create_tcpdump_filter [#VMs(m)]"
        exit 1
    fi
    local m=$1

    dbg_print "Creating ${TCPDUMP_FILTER} for ${m} VMs"

    if [ -f ${TCPDUMP_FILTER} ] ; then 
        rm ${TCPDUMP_FILTER}
    fi

    local slaves=() i
    for ((i = 1; i <= $m; i++)) ; do
        slaves+=(`printf "slave%03d" $i`)
    done
    
    printf "dst " > ${TCPDUMP_FILTER}
    for ((i = 0; i < ${#slaves[@]}; i++)) ; do 
        if [ $i -eq $((${#slaves[@]} - 1)) ] ; then
            printf "${slaves[$i]}\n" >> ${TCPDUMP_FILTER}
        else
            printf "${slaves[$i]} or " >> ${TCPDUMP_FILTER}
        fi
    done
}


function do_test() {
    if [ $# -ne 2 ] ; then
        dbg_print "Usage: do_test [app] [output directory]"
        exit 1
    fi

    local app=$1
    local out_dir=$2
    local comps=()

    case $app in
        rsort)
            comps=("spout" "sort")
            ;;
        sol)
            comps=("spout" "bolt1" "bolt2")        
            ;;
        wc)
            comps=("spout" "split" "count")
            ;;
        clean)
            comps=("spout" "view" "filter")
            ;;
        grep)
            comps=("spout" "find" "count")
            ;;
        pv)
            comps=("spout" "view" "count")
            ;;
        visitor)
            comps=("spout" "view" "uniquer")
            ;;
        rcount)
            comps=("spout" "split" "rolling_count")
            ;;
        vhtree)
            comps=(
                "PrequentialSourceProcessor" "EvaluatorProcessor"
                "FilterProcessor" "LocalStatisticsProcessor"
                "ModelAggregatorProcessor")
            ;;
    esac

    dbg_print "Getting topology info for ${app}..."

    run_cmd "wget -q http://${STORM_UI_SERVER}/api/v1/topology/summary"
    local summary
    if [ -f "summary" ] ; then
        summary="summary"
    else
        dbg_print "Summary file not found. Abort."
        exit 1        
    fi

    local topo_id=`cat ${summary} | jq '.topologies[].id' | tr -d '"'`
    if [[ ${topo_id} == "" ]] ; then
        dbg_print "Topology is not running. Abort."
        exit 1
    fi

    dbg_print "Topology ID: ${topo_id}"
    local topo_out_dir=${out_dir}/${topo_id}
    mkdir -p ${topo_out_dir}
    mv ${summary} ${topo_out_dir}

    run_cmd "wget -q http://${STORM_UI_SERVER}/api/v1/topology/${topo_id}"
    if [ ${#comps[@]} -eq 0 ] ; then
        comps=(`cat ${topo_id} | jq '.spouts[].spoutId' | tr -d '"'`)
        comps+=(`cat ${topo_id} | jq '.bolts[].boltId' | tr -d '"'`)
    fi
    mv ${topo_id} ${topo_out_dir}

    for comp in ${comps[@]}; do
        run_cmd "wget -q http://${STORM_UI_SERVER}/api/v1/topology/${topo_id}/component/'${comp}'"
        mv $comp ${topo_out_dir}
    done

    dbg_print "Start waiting for throughput to converge"
    monitor_throughput ${topo_out_dir}

    local i
    for ((i=1; i <= ${NUM_SAMPLING}; i++)) ; do
        dbg_print "Sampling metrics ($i/${NUM_SAMPLING})..."
        sample_metrics ${topo_out_dir}/$i
        local kafka_throughput=`tail -n1 ${topo_out_dir}/$i/tcpdump.log | cut -d' ' -f9`
        dbg_print "Kafka throughput = ${kafka_throughput}"
        test_results+=(${kafka_throughput})

        if [ $i -eq ${NUM_SAMPLING} ] ; then
            break
        fi

        dbg_print "Wait for next sampling: ${SAMPLING_INTERVAL_SEC} seconds"
        sleep ${SAMPLING_INTERVAL_SEC}
    done
}


function run_test {
    if [ $# -ne 2 ] ; then
        dbg_print "Usage; ./run {rsort|sol|wc|clean|grep|pv|visitor|rcount} [#VMs(m)]"
        exit 1    
    fi
    
    local app=$1 m=$2
    dbg_print "Start testing ${app}, ${m} VMs"

    local out_dir=${OUTPUT_DIR}/${app}/`printf "%03dvm" $m`
    dbg_print "Creating output dir: ${out_dir}"
    mkdir -p ${out_dir}

    local i
    for ((i=1; i <= $NUM_TESTS; i++)) ; do
        dbg_print "======== Starting test ($i/${NUM_TESTS}) ========"
        run_topo $app $m

        dbg_print "Post run $app wait: ${POST_RUN_TOPO_WAIT_SEC} seconds"
        sleep ${POST_RUN_TOPO_WAIT_SEC}

        do_test $app ${out_dir}

        dbg_print "======== Finishing test ($i/${NUM_TESTS}) ========"
        kill_topo

        if [ $i -eq $NUM_TESTS ] ; then
            break
        fi

        dbg_print "Post kill $app wait: ${POST_KILL_TOPO_WAIT_SEC} seconds"
        sleep ${POST_KILL_TOPO_WAIT_SEC}
    done

    dbg_print "Done testing ${app}, ${m} VMs"
    echo -e "$app\t$m\t`join_by $'\t' ${test_results[@]}`" >> ${out_dir}/results
    dbg_print "Wrote results to ${out_dir}/results"
}

function stop_instances {
    local instances=()
    instances+=(`aws ec2 describe-instances --region us-east-1 --filters Name=instance-state-name,Values=running | jq '.Reservations[].Instances[].InstanceId' -r`)
    echo "Stopping running instances ${instances[@]}..."
    aws ec2 stop-instances --instance-ids ${instances[@]}
}

function stop_kafka {
    run_cmd "ssh ${REMOTE_USER}@${KAFKA_HOST} \"${KAFKA_SERVER_STOP}\""
}

function stop_zkserver {
    run_cmd "ssh ${REMOTE_USER}@${ZKSERVER_HOST} \"${ZKSERVER_STOP}\""
}

