#!/bin/bash

source ./bench.conf

STORM_BENCH_JAR="${STORM_BENCH_HOME}/target/storm-bench-0.1-jar-with-dependencies.jar"
CONF="${STORM_BENCH_HOME}/conf/conf.yaml"
PACKAGE="rpi.storm.benchmark"
CORES_PER_VM=2

SAMOA_JAR="${SAMOA_HOME}/target/SAMOA-Storm-0.5.0-incubating-SNAPSHOT.jar"

NUM_TESTS=2     # test per app
NUM_SAMPLING=3  # sampling per test
SAMPLING_PERIOD_SEC=15
SAMPLING_INTERVAL_SEC=20

HOSTS_FILE=hosts
TCPDUMP_FILTER=tcpdump_filter
TCPDUMP="${REMOTE_DIR}/tcpdump"
SYSMON="${REMOTE_DIR}/sysmon"

POST_RUN_SUPERVISOR_WAIT_SEC=20
POST_KILL_SUPERVISOR_WAIT_SEC=10
POST_RUN_TOPO_WAIT_SEC=90
POST_KILL_TOPO_WAIT_SEC=15


###### misc functions

function join_by { local IFS="$1"; shift; echo "$*"; }
function get_date { date "+%Y-%m-%d %H:%M:%S"; }
function get_date_sec { date "+%s"; }
function dbg_print { echo "`get_date`" $1; }
function run_cmd { dbg_print "$1"; eval $1; }
function run_pssh {
    local remote_cmd=$1
    local cmd="pssh -l ${REMOTE_USER} -h ${HOSTS_FILE} \"${remote_cmd}\""
    run_cmd "$cmd"
}


###### run_supervisors & kill_supervisors

function run_supervisors {
    local m
    if [ $# -eq 0 ] || [ ${#1} -gt 3 ] ; then
        dbg_print "Usage: ./run_supervisor [1-128]"
        exit 1
    elif [ ${#1} -eq 1 ] ; then
        m=00$1
    elif [ ${#1} -eq 2 ] ; then
        m=0$1
    elif [ ${#1} -eq 3 ] ; then
        m=$1
    fi
    dbg_print "Starting $m supervisors..."
    run_cmd "pdsh -R ssh -l ${REMOTE_USER} -w slave[001-$m] 'sudo rm -rf /app/storm; sudo mkdir -p /app/storm; sudo chown -R ${REMOTE_USER}:${REMOTE_USER} /app/storm; /usr/local/storm/bin/storm supervisor < /dev/null > /home/${REMOTE_USER}/supervisor.log 2>&1 &'"
    echo $m > ./supervisors
}


function kill_supervisors {
    local m
    m=`cat ./supervisors`
    dbg_print "Killing ${m} supervisors..."
    run_cmd "pdsh -R ssh -l ${REMOTE_USER} -w slave[001-$m] 'pkill java'"
}


###### run_topo & kill_topo

function run_topo {
    if [ $# -ne 2 ] ; then
        dbg_print "Usage: run_topo {rsort|sol|wc|clean|grep|pv|visitor|rcount|vhtree} [#VMs(m)]"
        exit 1    
    fi

    local class
    local topo
    local topic
    local num_comps

    local is_samoa=false
    local num_classes

    local app=$1
    local m=$2

    case "$app" in
        rsort)
            class=RollingSort
            topo=rollingsort
            topic=rand
            num_comps=2
            ;;
        sol)
            class=SOL
            topo=sol
            topic=bytes
            num_comps=3
            ;;
        wc)
            class=WordCount
            topo=wordcount
            topic=book
            num_comps=3
            ;;
        clean)
            class=DataClean
            topo=dataclean
            topic=view
            num_comps=3
            ;;
        grep)
            class=Grep
            topo=grep
            topic=book
            num_comps=3
            ;;
        pv)
            class=PageViewCount
            topo=pageviewcount
            topic=view
            num_comps=3
            ;;
        visitor)
            class=UniqueVisitor
            topo=uniquevisitor
            topic=view
            num_comps=3
            ;;
        rcount)
            class=RollingCount
            topo=rollingcount
            topic=book
            num_comps=3
            ;;
        vhtree)
            topo=vhtree
            topic=covtype
            is_samoa=true
            num_classes=7
    esac

    dbg_print "Submitting ${topo}..."
    
    if [ ${is_samoa} == "false" ] ; then
        # Intel storm bench
        local parallel=$(($m * ${CORES_PER_VM} / ${num_comps}))
        if [ $parallel -lt 1 ] ; then
            parallel=1
        fi;
        local workers=$m
        local ackers=$m

        run_cmd "storm jar ${STORM_BENCH_JAR} ${PACKAGE}.${class} ${topo} -conf ${CONF} -topic ${topic} -parallel ${parallel} -workers ${workers} -ackers ${ackers}"
    else
        # SAMOA - only works for PrequentialEvaluation for the moment
        local parallel=$(($m - 4))
        if [ $parallel -lt 1 ] ; then
            parallel=1
        fi;
        export SAMOA_STORM_NUMWORKER=$m

        run_cmd "${SAMOA_HOME}/bin/samoa storm ${SAMOA_JAR} \"PrequentialEvaluation -d /tmp/dump.csv -i -1 -f 100000 -n ${topo} -l (classifiers.trees.VerticalHoeffdingTree -p ${parallel}) -s (org.apache.samoa.streams.kafka.KafkaStream -r 20 -t ${topic} -k $((${num_classes}+1)) -s ${KAFKA_HOST})\""
    fi

    echo ${topo} > ./topo
}


function kill_topo {
    local topo
    case "$1" in
        rsort)
            topo=rollingsort
            ;;
        sol)
            topo=sol
            ;;
        wc)
            topo=wordcount
            ;;
        clean)
            topo=dataclean
            ;;
        grep)
            topo=grep
            ;;
        pv)
            topo=pageviewcount
            ;;
        visitor)
            topo=uniquevisitorp
            ;;
        trident)
            topo=tridentwordcount
            ;;
        rcount)
            topo=rollingcount
            ;;
        preq)
            topo=prequential
            ;;
        *)
            topo=`cat ./topo`
            ;;
    esac

    dbg_print "Killing ${topo}..."
    run_cmd "storm kill -w 0 ${topo} || true"
}


###### sample_metrics

function sample_metrics {
    local output_dir
    if [ $# -eq 0 ] ; then
        output_dir="output"
    else
        output_dir=$1
    fi
    
    dbg_print "Start sampling (output_dir: ${output_dir})"
    run_cmd "ssh ${REMOTE_USER}@${TCPDUMP_HOST} \"sudo ${TCPDUMP} -i eth0 -Q out -F ${TCPDUMP_FILTER} --throughput-tracking-mode >& tcpdump.log &\""
    run_pssh "sudo ${SYSMON} 0 ${REMOTE_DIR}/sysmon.log 0x48040020 10000 2 > /dev/null &"

    dbg_print 'Monitoring in progress...'
    sleep ${SAMPLING_PERIOD_SEC}

    run_cmd "ssh ${REMOTE_USER}@${TCPDUMP_HOST} \"sudo pkill --signal 2 tcpdump\""
    run_pssh "sudo pkill --signal 2 sysmon"
    dbg_print 'Collecting metrics...'

    if [ ! -d "${output_dir}" ]; then
        mkdir -p ${output_dir}
    else
        rm -rf ${output_dir}/*
    fi

    run_cmd "pslurp -l ${REMOTE_USER} -h ${HOSTS_FILE} -L ${output_dir} ${REMOTE_DIR}/sysmon.log ."
    run_cmd "scp ${REMOTE_USER}@${TCPDUMP_HOST}:${REMOTE_DIR}/tcpdump.log ${output_dir}/"
}


###### run_test

function create_hosts_file {
    if [ $# -ne 1 ] ; then
        dbg_print "Usage: create_hosts_file [#VMs(m)]"
        exit 1
    fi

    local m=$1

    dbg_print "Creating ${HOSTS_FILE} file for ${m} VMs"

    if [ -f ${HOSTS_FILE} ] ; then 
        rm ${HOSTS_FILE}
    fi
    touch ${HOSTS_FILE}

    local host i
    for host in ${HOSTS[@]}; do
        echo $host >> ${HOSTS_FILE}
    done
    for ((i=1; i <= $m; i++)) ; do
        printf "slave%03d\n" $i >> ${HOSTS_FILE}
    done
}


function create_tcpdump_filter {
    if [ $# -ne 1 ] ; then
        dbg_print "Usage: create_tcpdump_filter [#VMs(m)]"
        exit 1
    fi
    local m=$1

    dbg_print "Creating ${TCPDUMP_FILTER} for ${m} VMs"

    if [ -f ${TCPDUMP_FILTER} ] ; then 
        rm ${TCPDUMP_FILTER}
    fi

    local slaves=() i
    for ((i = 1; i <= $m; i++)) ; do
        slaves+=(`printf "slave%03d" $i`)
    done
    
    printf "dst " > ${TCPDUMP_FILTER}
    for ((i = 0; i < ${#slaves[@]}; i++)) ; do 
        if [ $i -eq $((${#slaves[@]} - 1)) ] ; then
            printf "${slaves[$i]}\n" >> ${TCPDUMP_FILTER}
        else
            printf "${slaves[$i]} or " >> ${TCPDUMP_FILTER}
        fi
    done
}


function do_test() {
    if [ $# -ne 2 ] ; then
        dbg_print "Usage: do_test [app] [output directory]"
        exit 1
    fi

    local app=$1
    local out_dir=$2
    local comps=()

    case $app in
        rsort)
            comps=("spout" "sort")
            ;;
        sol)
            comps=("spout" "bolt1" "bolt2")        
            ;;
        wc)
            comps=("spout" "split" "count")
            ;;
        clean)
            comps=("spout" "view" "filter")
            ;;
        grep)
            comps=("spout" "find" "count")
            ;;
        pv)
            comps=("spout" "view" "count")
            ;;
        visitor)
            comps=("spout" "view" "uniquer")
            ;;
        rcount)
            comps=("spout" "split" "rolling_count")
            ;;
        *)
            comps=()
            ;;
    esac

    if [ ${#comps[@]} -gt 0 ] ; then
        dbg_print "Getting topology info for ${app}..."

        run_cmd "wget -q http://${STORM_UI_SERVER}/api/v1/topology/summary"
        local summary
        # summary can be saved as summary.1
        if [ -f "summary" ] ; then
            summary="summary"
        elif [ -f "summary.1" ] ; then
            summary="summary.1"
        else
            dbg_print "Summary file not found. Abort."
            exit 1        
        fi

        local topo_id=`cat ${summary} | jq '.topologies[].id' | tr -d '"'`
        if [[ ${topo_id} == "" ]] ; then
            dbg_print "Topology is not running. Abort."
            exit 1
        fi

        dbg_print "Topology ID: ${topo_id}"
        local topo_out_dir=${out_dir}/${topo_id}
        mkdir -p ${topo_out_dir}
        mv ${summary} ${topo_out_dir}

        for comp in ${comps[@]}; do
            run_cmd "wget -q http://${STORM_UI_SERVER}/api/v1/topology/${topo_id}/component/${comp}"
            mv $comp ${topo_out_dir}
        done
    else
        # Primarily for SAMOA apps
        topo_id=$app-`get_date_sec`
        dbg_print "Topology ID: ${topo_id}"
        local topo_out_dir=${out_dir}/${topo_id}
        mkdir -p ${topo_out_dir}
    fi

    local i
    for ((i=1; i <= ${NUM_SAMPLING}; i++)) ; do
        dbg_print "Sampling metrics ($i/${NUM_SAMPLING})..."
        sample_metrics ${topo_out_dir}/$i
        local kafka_throughput=`tail -n1 ${topo_out_dir}/$i/tcpdump.log | cut -d' ' -f9`
        dbg_print "Kafka throughput = ${kafka_throughput}"
        test_results+=(${kafka_throughput})

        if [ $i -eq ${NUM_SAMPLING} ] ; then
            break
        fi

        dbg_print "Wait for next sampling: ${SAMPLING_INTERVAL_SEC} seconds"
        sleep ${SAMPLING_INTERVAL_SEC}
    done
}


function run_test {
    if [ $# -ne 2 ] ; then
        dbg_print "Usage; ./run {rsort|sol|wc|clean|grep|pv|visitor|rcount} [#VMs(m)]"
        exit 1    
    fi
    
    local app=$1 m=$2
    dbg_print "Start testing ${app}, ${m} VMs"

    local out_dir=${OUTPUT_DIR}/${app}/`printf "%03dvm" $m`
    dbg_print "Creating output dir: ${out_dir}"
    mkdir -p ${out_dir}

    local i
    for ((i=1; i <= $NUM_TESTS; i++)) ; do
        dbg_print "======== Starting test ($i/${NUM_TESTS}) ========"
        run_topo $app $m

        dbg_print "Post run $app wait: ${POST_RUN_TOPO_WAIT_SEC} seconds"
        sleep ${POST_RUN_TOPO_WAIT_SEC}

        do_test $app ${out_dir}

        dbg_print "======== Finishing test ($i/${NUM_TESTS}) ========"
        kill_topo

        if [ $i -eq $NUM_TESTS ] ; then
            break
        fi

        dbg_print "Post kill $app wait: ${POST_KILL_TOPO_WAIT_SEC} seconds"
        sleep ${POST_KILL_TOPO_WAIT_SEC}
    done

    dbg_print "Done testing ${app}, ${m} VMs"
    echo -e "$app\t$m\t`join_by $'\t' ${test_results[@]}`" >> ${out_dir}/results
    dbg_print "Wrote results to ${out_dir}/results"
}

function stop_instances {
    local instances=()
    instances+=(`aws ec2 describe-instances --region us-east-1 --filters Name=instance-state-name,Values=running | jq '.Reservations[].Instances[].InstanceId' -r`)
    echo "Stopping running instances ${instances[@]}..."
    aws ec2 stop-instances --instance-ids ${instances[@]}
}

function stop_kafka {
    run_cmd "ssh ${REMOTE_USER}@${KAFKA_HOST} \"${KAFKA_SERVER_STOP}\""
}

function stop_zkserver {
    run_cmd "ssh ${REMOTE_USER}@${ZKSERVER_HOST} \"${ZKSERVER_STOP}\""
}
